{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask2Former.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimKhan5x/image-search-analysis-dip/blob/main/Metadata%20Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output\n",
        "!mkdir input\n",
        "!mkdir predictions\n",
        "!wget https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/panoptic/maskformer2_swin_large_IN21k_384_bs16_100ep/model_final_f07440.pkl\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!git clone https://github.com/QasimKhan5x/Mask2Former\n",
        "%cd Mask2Former\n",
        "!pip install -r requirements.txt\n",
        "%cd ./mask2former/modeling/pixel_decoder/ops\n",
        "!sh make.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "nkDH-XEf0CSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q deepface"
      ],
      "metadata": {
        "id": "BVTmjGq2Xcrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejRv0z5-G4Di",
        "outputId": "bbfcff33-fe85-4fbf-83ef-859d740a030f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "from scipy.io import loadmat\n",
        "\n",
        "# Person Model\n",
        "from deepface import DeepFace\n",
        "from tensorflow.python.keras.applications.resnet50 import preprocess_input as preprocess_dog"
      ],
      "metadata": {
        "id": "vMupSZd5ehim"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask2Former Panoptic Segmentation\n",
        "\n",
        "preds_dst = '/content/predictions/'\n",
        "config_file = '/content/Mask2Former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml'\n",
        "m2f_weights = '/content/model_final_f07440.pkl'\n",
        "coco_anns = json.loads(requests.get('https://raw.githubusercontent.com/cocodataset/panopticapi/master/panoptic_coco_categories.json').text)"
      ],
      "metadata": {
        "id": "FeF9mQl6gkDS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Car Model\n",
        "\n",
        "car_anns = loadmat('/content/drive/MyDrive/Attribute Detection/Car/cars_annos.mat')['class_names']\n",
        "car_anns = np.concatenate(car_anns['class_names'].flatten()).tolist()\n",
        "\n",
        "ckpt = '/content/drive/MyDrive/Attribute Detection/Car/car.pt'\n",
        "car_model = torch.load(ckpt).eval()"
      ],
      "metadata": {
        "id": "rqs6_-W-whCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cat Model\n",
        "dog_model = DogModel().eval()\n"
      ],
      "metadata": {
        "id": "3qhCZzrFq_Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Mask2Former/demo\n",
        "!python demo.py --config-file /content/Mask2Former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml \\\n",
        "--input /content/input/tv_image05.png \\\n",
        "--preds_dest /content/predictions/tv_image05 \\\n",
        "--output /content/output \\\n",
        "--opts MODEL.WEIGHTS /content/model_final_f07440.pkl\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "GH-MSRczLhuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def panoptic_segment(img_path, config_file, weights):\n",
        "    %cd /content/Mask2Former/demo\n",
        "    dst_fn = Path(img_path).stem\n",
        "    dst_fp = os.path.join(preds_dst, dst_fn)\n",
        "    cmd = f'python demo.py --config-file {config_file} ' \\\n",
        "          f'--input {img_path} ' \\\n",
        "          f'--preds_dest {dst_fp} ' \\\n",
        "          '--output /content/output ' \\\n",
        "          f'--opts MODEL.WEIGHTS {weights}'\n",
        "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
        "    out, err = p.communicate()\n",
        "    %cd /content"
      ],
      "metadata": {
        "id": "bDqpsZ4607qZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1 = \"/content/input/tv_image05.png\"\n",
        "i2 = \"/content/input/2008_000880.jpg\"\n",
        "\n",
        "img1 = Image.open(i1).convert('RGB')\n",
        "img2 = Image.open(i2).convert('RGB')\n",
        "\n",
        "panoptic_segment(i1, config_file, m2f_weights)\n",
        "panoptic_segment(i2, config_file, m2f_weights)\n",
        "\n",
        "o1 = '/content/output/tv_image05.png'\n",
        "o2 = '/content/output/2008_000880.jpg'\n",
        "\n",
        "out1 = Image.open(o1).convert('RGB')\n",
        "out2 = Image.open(o2).convert('RGB')"
      ],
      "metadata": {
        "id": "QG1CDakeew0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4d7792-1899-4c9c-fd88-7d17d80869b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former/demo\n",
            "/content\n",
            "/content/Mask2Former/demo\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes**\n",
        "\n",
        "1. Object should occupy atleast 1% of image."
      ],
      "metadata": {
        "id": "o_YVWVBXjNJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_thresh(img):\n",
        "    return img.size[0] * img.size[1] * 0.01\n",
        "\n",
        "def get_bounding_box(img):\n",
        "    # region of interest\n",
        "    roi = np.argwhere(img == 255)\n",
        "    # starting point --> top left corner\n",
        "    y1, x1 = roi[:, 0].min(), roi[:, 1].min()\n",
        "    # ending point --> bottom right corner\n",
        "    y2, x2 = roi[:, 0].max(), roi[:, 1].max()\n",
        "    return (x1, y1), (x2, y2)\n",
        "\n",
        "def apply_bounding_box(img):\n",
        "    start, end = get_bounding_box(img)\n",
        "    rect = cv2.rectangle(img, start, end, (255, 0, 0), 1)\n",
        "    return rect.astype('uint8')"
      ],
      "metadata": {
        "id": "a-U5DfrJpLVJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metadata(img, labels, instances):\n",
        "    img_rgb = np.asarray(img)\n",
        "    thresh = get_thresh(img)\n",
        "    img_metadata = list()\n",
        "    global coco_anns\n",
        "\n",
        "    for instance in instances:\n",
        "        if instance['area'] <= thresh or not instance['isthing']:\n",
        "            continue\n",
        "        instance_id = instance['id']\n",
        "        cat_id = instance['category_id']\n",
        "        \n",
        "        metadata = dict()\n",
        "        supercategory = coco_anns[cat_id]['supercategory']\n",
        "        name = coco_anns[cat_id]['name']\n",
        "        metadata['supercategory'] = supercategory\n",
        "        metadata['category'] = name\n",
        "\n",
        "        # get region of interest for current instance\n",
        "        roi = np.where(labels == instance_id, 255, 0).astype('uint8')\n",
        "        (x1, y1), (x2, y2) = get_bounding_box(roi)\n",
        "        crop_rgb = np.zeros(img_rgb.shape, dtype='uint8')\n",
        "        crop_rgb[y1:y2, x1:x2] = img_rgb[y1:y2, x1:x2]\n",
        "        # crop\n",
        "        crop_rgb = crop_rgb[y1:y2, x1:x2]\n",
        "        metadata['image'] = crop_rgb\n",
        "        img_metadata.append(metadata)\n",
        "    return img_metadata"
      ],
      "metadata": {
        "id": "Zzm1C-fJl6Jj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_metadata_from_imgpath(imgpath):\n",
        "    global config_file\n",
        "    global m2f_weights\n",
        "    panoptic_segment(imgpath, config_file, m2f_weights)\n",
        "    filename = os.path.basename(imgpath)\n",
        "    filestem = filename.split('.')[0]\n",
        "    preds_fp = '/content/predictions/' + filestem + \".pkl\"\n",
        "    with open(preds_fp, \"rb\") as f:\n",
        "        preds = pickle.load(f)['panoptic_seg']\n",
        "    img = Image.open(imgpath).convert(\"RGB\")\n",
        "    labels = preds[0].cpu().detach().numpy()\n",
        "    instances = preds[1]\n",
        "    metadata = get_metadata(img, labels, instances)\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "Po9cnETatZeZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{\n",
        "    \"supercategory\":,\n",
        "    \"category\":,\n",
        "    \"attributes\": [\n",
        "                   \"make\":,\n",
        "                   \"model\":,\n",
        "                   \"year\":,\n",
        "    ],\n",
        "    \"milvus_id\": 1\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "jFXWWfkqUV8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = create_metadata_from_imgpath('/content/input/2008_000880.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Kb7tCOPXHV",
        "outputId": "ddc51b31-c4e1-47e7-f548-98cbbe166b9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former/demo\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Bokq4GRXAZ",
        "outputId": "cd900ad8-bf5e-42e2-ae40-0419b98dab7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata[0]['name']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zoidmvt-c4td",
        "outputId": "1681a99a-7118-46f2-b087-cc388d616404"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'car'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attributes"
      ],
      "metadata": {
        "id": "GHG4o83CRMja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_person_attributes(img):\n",
        "    '''\n",
        "    Return attributes if face found\n",
        "    Else return {}\n",
        "    '''\n",
        "    # if face not found\n",
        "    detectors = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface']\n",
        "    face_found = False\n",
        "    for detector in detectors:\n",
        "        try:\n",
        "            img = DeepFace.detectFace(img, detector_backend = detector)   \n",
        "            face_found = True\n",
        "            break\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not face_found:\n",
        "        return {}\n",
        "\n",
        "    demography = DeepFace.analyze(img, ['age', 'gender', 'race', 'emotion'], enforce_detection=False)\n",
        "\n",
        "    if demography['age'] < 13:\n",
        "        age_cat = 'child'\n",
        "    elif demography['age'] < 20:\n",
        "        age_cat = 'teenager'\n",
        "    elif demography['age'] < 40:\n",
        "        age_cat = 'adult'\n",
        "    elif demography['age'] < 65:\n",
        "        age_cat = 'middle_aged'\n",
        "    else:\n",
        "        age_cat = 'old'\n",
        "\n",
        "    attributes = {\n",
        "        'gender': demography[\"gender\"],\n",
        "        'race': demography[\"dominant_race\"],\n",
        "        'emotion': demography[\"dominant_emotion\"],\n",
        "        'age': age_cat\n",
        "    }\n",
        "\n",
        "    return attributes"
      ],
      "metadata": {
        "id": "AdVokZ7HWDV5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cat_attributes(img):\n",
        "    pass"
      ],
      "metadata": {
        "id": "Yc-gX8n0dwsT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_car_attributes(img):\n",
        "    transform = T.Compose([T.Resize((400, 400)),\n",
        "                           T.ToTensor(),\n",
        "                           T.Normalize((0.5, 0.5, 0.5), \n",
        "                                       (0.5, 0.5, 0.5))])\n",
        "    \n",
        "    tensor = transform(img).float().unsqueeze(0)\n",
        "    if torch.cuda.is_avalable():\n",
        "        tensor = tensor.cuda()\n",
        "    \n",
        "    output = car_model(image)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    label = car_anns[predicted.item()]\n",
        "\n",
        "    # separate by spaces\n",
        "    # 1st token -> make\n",
        "    # 2nd to penultimate -> model\n",
        "    # last -> year\n",
        "    # fails if make consists of more than 1 token\n",
        "    tokens = label.split()\n",
        "\n",
        "    return {\n",
        "        'make': tokens[0],\n",
        "        'model': \" \".join(tokens[1:-1])\n",
        "        'year': int(tokens[-1])\n",
        "    }"
      ],
      "metadata": {
        "id": "oKO2rU5wju71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_attributes_to_metadata(metadata):\n",
        "    for instance in metadata:\n",
        "        img = Image.fromarray(instance['image'])\n",
        "        if image['category'] == 'person':\n",
        "            image['metadata'] = get_person_attributes(img)\n",
        "        elif image['category'] == 'cat':\n",
        "            image['metadata'] = get_cat_attributes(img)\n",
        "        elif image['category'] == 'car':\n",
        "            image['metadata'] = get_car_attributes(img)\n",
        "        else:\n",
        "            image['metadata'] = {}\n",
        "        # the image is no longer needed\n",
        "        del instance['image']\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "RzEgGyQPz9JG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_metadata_from_imgpath(imgpath):\n",
        "    metadata = create_base_metadata_from_imgpath(imgpath)\n",
        "    metadata_with_attributes = add_attributes_to_metadata(metadata)\n",
        "    return metadata_with_attributes"
      ],
      "metadata": {
        "id": "0zF84xX809Wf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}