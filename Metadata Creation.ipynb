{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metadata Creation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimKhan5x/image-search-analysis-dip/blob/main/Metadata%20Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Creation (2nd Milestone of DIP Project)\n",
        "\n",
        "This notebook is used to take an image as input and create a dictionary that contains metadata about the image. This metadata contains information related to the objects in that image as well as some attributes that describe specific objects.\n",
        "\n",
        "**Sample Metadata Object Structure**\n",
        "```\n",
        "[\n",
        "    34534543, # milvus_id\n",
        "    {\n",
        "    \"supercategory\": \"vehicle\",\n",
        "    \"category\": \"car\",\n",
        "    \"attributes\": [\n",
        "                   \"make\": \"Mercedes-Benz\",\n",
        "                   \"model\": \"C-Class Sedan\",\n",
        "                   \"year\": 2012,\n",
        "                   ],\n",
        "    \"milvus_id\": 1\n",
        "    },\n",
        "]\n",
        "\n",
        "```\n",
        "\n",
        "This metadata is useful for improving the precision of a vector similarity search engine. The process is as follows:\n",
        "\n",
        "1. Create metadata for all the images in your dataset\n",
        "2. Store that metadata in a NoSQL database (MongoDB) or JSON file\n",
        "3. When performing reverse image search, create metadata for your image\n",
        "4. Query your vector embeddings collection to only find vectors that contain one or more of the objects in your metadata"
      ],
      "metadata": {
        "id": "6b3zXLxd3NSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf7gIJxO5SW2",
        "outputId": "4cf94d48-1dcf-4cb3-9f38-e2359530ec58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations, Imports, Model Declaration"
      ],
      "metadata": {
        "id": "ceYMkF1r5LuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output\n",
        "!mkdir input\n",
        "!mkdir predictions\n",
        "!wget https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/panoptic/maskformer2_swin_large_IN21k_384_bs16_100ep/model_final_f07440.pkl\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install -q deepface\n",
        "!git clone https://github.com/QasimKhan5x/Mask2Former\n",
        "%cd Mask2Former\n",
        "!pip install -r requirements.txt\n",
        "%cd ./mask2former/modeling/pixel_decoder/ops\n",
        "!sh make.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "nkDH-XEf0CSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "from scipy.io import loadmat\n",
        "\n",
        "# Person Model\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Cat Model"
      ],
      "metadata": {
        "id": "vMupSZd5ehim"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask2Former Panoptic Segmentation\n",
        "\n",
        "preds_dst = '/content/predictions/'\n",
        "config_file = '/content/Mask2Former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml'\n",
        "m2f_weights = '/content/model_final_f07440.pkl'\n",
        "coco_anns = json.loads(requests.get('https://raw.githubusercontent.com/cocodataset/panopticapi/master/panoptic_coco_categories.json').text)"
      ],
      "metadata": {
        "id": "FeF9mQl6gkDS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Car Model\n",
        "\n",
        "# stores the annotations as a 2D array in which axis=1 contains single-item arrays\n",
        "car_anns = loadmat('/content/drive/MyDrive/AttributeDetection/Car/cars_annos.mat')['class_names']\n",
        "# converts the annotations to an intelligible list\n",
        "car_anns = np.concatenate(car_anns['class_names'].flatten()).tolist()\n",
        "\n",
        "ckpt = '/content/drive/MyDrive/AttributeDetection/Car/car.pt'\n",
        "car_model = torch.load(ckpt).eval()"
      ],
      "metadata": {
        "id": "rqs6_-W-whCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cat Model\n",
        "cat_model = CatModel().eval()"
      ],
      "metadata": {
        "id": "3qhCZzrFq_Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Panoptic Segmentation \n",
        "\n",
        "This section contains code to perform panoptic segmentation with Mask2Former and create an initial metadata object without attributes of objects.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "1. Object should occupy atleast 1% of image."
      ],
      "metadata": {
        "id": "psQZq-c85tlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample command to run Mask2Former on an image, stores its output in /content/output, and store the predictions dictionary in /content/predictions"
      ],
      "metadata": {
        "id": "AE7HqWBD5dGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "%cd /content/Mask2Former/demo\n",
        "!python demo.py --config-file /content/Mask2Former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml \\\n",
        "--input /content/input/tv_image05.png \\\n",
        "--preds_dest /content/predictions/tv_image05 \\\n",
        "--output /content/output \\\n",
        "--opts MODEL.WEIGHTS /content/model_final_f07440.pkl\n",
        "%cd /content\n",
        "'''"
      ],
      "metadata": {
        "id": "GH-MSRczLhuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def panoptic_segment(img_path, config_file, weights):\n",
        "    '''\n",
        "    Provide the paths to\n",
        "        1. Image\n",
        "        2. Configuration file\n",
        "        3. Weights\n",
        "    Saves output in output and predictions directories\n",
        "    '''\n",
        "    %cd /content/Mask2Former/demo\n",
        "    dst_fn = Path(img_path).stem\n",
        "    dst_fp = os.path.join(preds_dst, dst_fn)\n",
        "    cmd = f'python demo.py --config-file {config_file} ' \\\n",
        "          f'--input {img_path} ' \\\n",
        "          f'--preds_dest {dst_fp} ' \\\n",
        "          '--output /content/output ' \\\n",
        "          f'--opts MODEL.WEIGHTS {weights}'\n",
        "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
        "    out, err = p.communicate()\n",
        "    %cd /content"
      ],
      "metadata": {
        "id": "bDqpsZ4607qZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_thresh(img):\n",
        "    '''Only consider objects whose size > 1% of image size'''\n",
        "    return img.size[0] * img.size[1] * 0.01\n",
        "\n",
        "def get_bounding_box(img):\n",
        "    '''Get locations of bounding box on an object'''\n",
        "    # region of interest\n",
        "    roi = np.argwhere(img == 255)\n",
        "    # starting point --> top left corner\n",
        "    y1, x1 = roi[:, 0].min(), roi[:, 1].min()\n",
        "    # ending point --> bottom right corner\n",
        "    y2, x2 = roi[:, 0].max(), roi[:, 1].max()\n",
        "    return (x1, y1), (x2, y2)\n",
        "\n",
        "def apply_bounding_box(img):\n",
        "    '''Draw a bounding box on the image'''\n",
        "    start, end = get_bounding_box(img)\n",
        "    rect = cv2.rectangle(img, start, end, (255, 0, 0), 1)\n",
        "    return rect.astype('uint8')"
      ],
      "metadata": {
        "id": "a-U5DfrJpLVJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metadata(img, labels, instances):\n",
        "    '''\n",
        "    For each object instance in the image,\n",
        "    create a metadata object that contains the\n",
        "        1. Image Supercategory\n",
        "        2. Image Category\n",
        "        3. Array representing the image\n",
        "    (3) is temporarily saved so that it can be\n",
        "    used for attributes prediction\n",
        "    (1) and (2) are obtained through COCO annotations\n",
        "    '''\n",
        "    img_rgb = np.asarray(img)\n",
        "    thresh = get_thresh(img)\n",
        "    img_metadata = list()\n",
        "    global coco_anns\n",
        "\n",
        "    for instance in instances:\n",
        "        if instance['area'] <= thresh or not instance['isthing']:\n",
        "            continue\n",
        "\n",
        "        instance_id = instance['id']\n",
        "        cat_id = instance['category_id']\n",
        "        \n",
        "        metadata = dict()\n",
        "        supercategory = coco_anns[cat_id]['supercategory']\n",
        "        name = coco_anns[cat_id]['name']\n",
        "        metadata['supercategory'] = supercategory\n",
        "        metadata['category'] = name\n",
        "\n",
        "        # get region of interest for current instance\n",
        "        roi = np.where(labels == instance_id, 255, 0).astype('uint8')\n",
        "        (x1, y1), (x2, y2) = get_bounding_box(roi)\n",
        "        crop_rgb = np.zeros(img_rgb.shape, dtype='uint8')\n",
        "        crop_rgb[y1:y2, x1:x2] = img_rgb[y1:y2, x1:x2]\n",
        "        # crop\n",
        "        crop_rgb = crop_rgb[y1:y2, x1:x2]\n",
        "        metadata['image'] = crop_rgb\n",
        "        img_metadata.append(metadata)\n",
        "    return img_metadata\n",
        "\n",
        "def create_base_metadata_from_imgpath(imgpath):\n",
        "    global config_file\n",
        "    global m2f_weights\n",
        "    panoptic_segment(imgpath, config_file, m2f_weights)\n",
        "    filename = os.path.basename(imgpath)\n",
        "    filestem = filename.split('.')[0]\n",
        "    preds_fp = '/content/predictions/' + filestem + \".pkl\"\n",
        "    with open(preds_fp, \"rb\") as f:\n",
        "        preds = pickle.load(f)['panoptic_seg']\n",
        "    img = Image.open(imgpath).convert(\"RGB\")\n",
        "    labels = preds[0].cpu().detach().numpy()\n",
        "    instances = preds[1]\n",
        "    metadata = get_metadata(img, labels, instances)\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "Zzm1C-fJl6Jj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "metadata = create_metadata_from_imgpath('/content/input/2008_000880.jpg')\n",
        "print(len(metadata))\n",
        "# Display cropped image of object\n",
        "Image.fromarray(metadata[0]['image'])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Kb7tCOPXHV",
        "outputId": "ddc51b31-c4e1-47e7-f548-98cbbe166b9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former/demo\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification"
      ],
      "metadata": {
        "id": "GHG4o83CRMja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_person_attributes(img):\n",
        "    '''\n",
        "    Return attributes if face found\n",
        "    Else return {}\n",
        "    '''\n",
        "    # if face not found\n",
        "    detectors = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface']\n",
        "    face_found = False\n",
        "    for detector in detectors:\n",
        "        try:\n",
        "            img = DeepFace.detectFace(img, detector_backend = detector)   \n",
        "            face_found = True\n",
        "            break\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not face_found:\n",
        "        return {}\n",
        "\n",
        "    demography = DeepFace.analyze(img, ['age', 'gender', 'race', 'emotion'], enforce_detection=False)\n",
        "\n",
        "    if demography['age'] < 13:\n",
        "        age_cat = 'child'\n",
        "    elif demography['age'] < 20:\n",
        "        age_cat = 'teenager'\n",
        "    elif demography['age'] < 40:\n",
        "        age_cat = 'adult'\n",
        "    elif demography['age'] < 65:\n",
        "        age_cat = 'middle_aged'\n",
        "    else:\n",
        "        age_cat = 'old'\n",
        "\n",
        "    attributes = {\n",
        "        'gender': demography[\"gender\"],\n",
        "        'race': demography[\"dominant_race\"],\n",
        "        'emotion': demography[\"dominant_emotion\"],\n",
        "        'age': age_cat\n",
        "    }\n",
        "\n",
        "    return attributes"
      ],
      "metadata": {
        "id": "AdVokZ7HWDV5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cat_attributes(img):\n",
        "    pass"
      ],
      "metadata": {
        "id": "Yc-gX8n0dwsT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_car_attributes(img):\n",
        "    transform = T.Compose([T.Resize((400, 400)),\n",
        "                           T.ToTensor(),\n",
        "                           T.Normalize((0.5, 0.5, 0.5), \n",
        "                                       (0.5, 0.5, 0.5))])\n",
        "    \n",
        "    tensor = transform(img).float().unsqueeze(0)\n",
        "    if torch.cuda.is_avalable():\n",
        "        tensor = tensor.cuda()\n",
        "    \n",
        "    output = car_model(image)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    label = car_anns[predicted.item()]\n",
        "\n",
        "    # separate by spaces\n",
        "    # 1st token -> make\n",
        "    # 2nd to penultimate -> model\n",
        "    # last -> year\n",
        "    # fails if make consists of more than 1 token\n",
        "    tokens = label.split()\n",
        "\n",
        "    return {\n",
        "        'make': tokens[0],\n",
        "        'model': \" \".join(tokens[1:-1])\n",
        "        'year': int(tokens[-1])\n",
        "    }"
      ],
      "metadata": {
        "id": "oKO2rU5wju71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_attributes_to_metadata(metadata):\n",
        "    '''\n",
        "    Apply available image classification models on the metadata\n",
        "    Currently, the following models are used:\n",
        "        1. Person - ethnicity, age, gender, mood\n",
        "        2. Cat - breed\n",
        "        3. Car - make, model, year\n",
        "    '''\n",
        "    for instance in metadata:\n",
        "        img = Image.fromarray(instance['image'])\n",
        "        if image['category'] == 'person':\n",
        "            image['metadata'] = get_person_attributes(img)\n",
        "        elif image['category'] == 'cat':\n",
        "            image['metadata'] = get_cat_attributes(img)\n",
        "        elif image['category'] == 'car':\n",
        "            image['metadata'] = get_car_attributes(img)\n",
        "        else:\n",
        "            image['metadata'] = {}\n",
        "        # the image is no longer needed\n",
        "        del instance['image']\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "RzEgGyQPz9JG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Metadata Creation"
      ],
      "metadata": {
        "id": "A-spX-qn7UXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_metadata_from_imgpath(imgpath):\n",
        "    metadata = create_base_metadata_from_imgpath(imgpath)\n",
        "    metadata_with_attributes = add_attributes_to_metadata(metadata)\n",
        "    return metadata_with_attributes"
      ],
      "metadata": {
        "id": "0zF84xX809Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update PASCAL VOC 2012\n",
        "\n",
        "This section contains the code to create metadata objects for the aforementioned dataset. This section is present because we are using VOC'12 for our project demo.\n",
        "\n",
        "Since we will be annotating our images with their respective `milvus_id`s as well, a JSON file containing the `milvus_id` for each image name is required."
      ],
      "metadata": {
        "id": "22e1QEdcAPyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/MyDrive/AttributeDetection/mid_filename.json'\n",
        "with open(filepath) as f:\n",
        "    imgname2mid = json.load(f)"
      ],
      "metadata": {
        "id": "ncRkA5CcAtdg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset from Kaggle"
      ],
      "metadata": {
        "id": "wvfXYwf3Bgff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle_creds = '/content/drive/MyDrive/AttributeDetection/kaggle.json'\n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp $kaggle_creds ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download huanghanchina/pascal-voc-2012\n",
        "!unzip pascal-voc-2012.zip"
      ],
      "metadata": {
        "id": "NVkI3dC-Es5X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMGS_DIR = '/content/VOC2012/JPEGImages'\n",
        "images = os.listdir()"
      ],
      "metadata": {
        "id": "o30oiIqJEcw8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img in images:\n",
        "    img_path = os.path.join(IMGS_DIR, img)\n",
        "    metadata = create_metadata_from_imgpath(img_path)\n",
        "    mid = imgname2mid[img]\n",
        "    metadata.insert(0, mid)\n",
        "with open(\"/content/metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)"
      ],
      "metadata": {
        "id": "czBZ0wE_FYgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b7YZ7RBhGVnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}