{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimKhan5x/image-search-analysis-dip/blob/main/Metadata_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3zXLxd3NSW"
      },
      "source": [
        "# Metadata Creation (2nd Milestone of DIP Project)\n",
        "\n",
        "This notebook is used to take an image as input and create a dictionary that contains metadata about the image. This metadata contains information related to the objects in that image as well as some attributes that describe specific objects.\n",
        "\n",
        "**Sample Metadata Object Structure**\n",
        "```\n",
        "[\n",
        "    {\n",
        "        \"milvus_id\": 123456,\n",
        "        \"name\": \"2007_001234.jpg\"\n",
        "    },\n",
        "    {\n",
        "    \"supercategory\": \"vehicle\",\n",
        "    \"category\": \"car\",\n",
        "    \"attributes\": [\n",
        "                   \"make\": \"Mercedes-Benz\",\n",
        "                   \"model\": \"C-Class Sedan\",\n",
        "                   \"year\": 2012,\n",
        "                   ],\n",
        "    },\n",
        "]\n",
        "\n",
        "```\n",
        "\n",
        "This metadata is useful for improving the precision of a vector similarity search engine. The process is as follows:\n",
        "\n",
        "1. Create metadata for all the images in your dataset\n",
        "2. Store that metadata in a NoSQL database (MongoDB) or JSON file\n",
        "3. When performing reverse image search, create metadata for your image\n",
        "4. Query your vector embeddings collection to only find vectors that contain one or more of the objects in your metadata\n",
        "\n",
        "Run all sections until **Final Metadata Creation**. The last section is exclusively to generate metadata for PASCAL VOC 2012.\n",
        "\n",
        "**Personal Notes**\n",
        "- If Mask2Former is unable to detect an object, say a person, then even if **DeepFace** can find that person, he/she will not be registered as metadata. Possible solution of above are:\n",
        "    1. Specifically for persons, use DeepFace and Mask2Former both\n",
        "    2. Finetune Mask2Former on a custom dataset (for future use)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceYMkF1r5LuK"
      },
      "source": [
        "## Installations, Imports, Model Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkDH-XEf0CSY"
      },
      "outputs": [],
      "source": [
        "!mkdir output\n",
        "!mkdir input\n",
        "!mkdir predictions\n",
        "!wget https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/panoptic/maskformer2_swin_large_IN21k_384_bs16_100ep/model_final_f07440.pkl\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install deepface gdown -U\n",
        "!gdown https://drive.google.com/drive/folders/1NYasnQ9aSG9KuFKO0moeN4JCwHfI7avq?usp=sharing -O /content/ --folder\n",
        "!git clone https://github.com/QasimKhan5x/Mask2Former\n",
        "%cd Mask2Former\n",
        "!pip install -r requirements.txt\n",
        "%cd ./mask2former/modeling/pixel_decoder/ops\n",
        "!sh make.sh\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vMupSZd5ehim"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import json\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "from scipy.io import loadmat\n",
        "\n",
        "# Dog Model\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel(3)\n",
        "from tensorflow.keras.models import load_model,Model\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
        "import pandas as pd \n",
        "\n",
        "# Hide GPU from visible devices\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "# Person Model\n",
        "from deepface import DeepFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FeF9mQl6gkDS"
      },
      "outputs": [],
      "source": [
        "# Mask2Former Panoptic Segmentation\n",
        "\n",
        "preds_dst = '/content/predictions/'\n",
        "config_file = '/content/Mask2Former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml'\n",
        "m2f_weights = '/content/model_final_f07440.pkl'\n",
        "coco_anns = json.loads(requests.get('https://raw.githubusercontent.com/cocodataset/panopticapi/master/panoptic_coco_categories.json').text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rqs6_-W-whCl"
      },
      "outputs": [],
      "source": [
        "# Car Model\n",
        "\n",
        "# stores the annotations as a 2D array in which axis=1 contains single-item arrays\n",
        "car_anns = loadmat('/content/AttributeDetection/Car/cars_annos.mat')['class_names']\n",
        "# converts the annotations to an intelligible list\n",
        "car_anns = np.concatenate(car_anns.flatten()).tolist()\n",
        "\n",
        "ckpt = '/content/AttributeDetection/Car/car.pt'\n",
        "car_model = torch.load(ckpt, map_location='cpu').eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qhCZzrFq_Ow"
      },
      "outputs": [],
      "source": [
        "# Dog Model\n",
        "\n",
        "# get labels of dog classes\n",
        "dog_labels = pd.read_csv(\"/content/AttributeDetection/labels.csv\")\n",
        "# get only 60 unique breeds record\n",
        "breed_dict = list(dog_labels['breed'].value_counts().keys()) \n",
        "num_breeds = 60\n",
        "new_list = sorted(breed_dict,reverse=True)[:num_breeds * 2 + 1:2]\n",
        "\n",
        "dog_model = tf.keras.models.load_model('/content/AttributeDetection/DBC.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psQZq-c85tlq"
      },
      "source": [
        "## Panoptic Segmentation \n",
        "\n",
        "This section contains code to perform panoptic segmentation with Mask2Former and create an initial metadata object without attributes of objects.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "1. Object should occupy atleast 1% of image. This comes out as a major drawback for several cases and needs to be revised. For example, objects that are inherently small will always occupy <1% of an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE7HqWBD5dGD"
      },
      "source": [
        "Sample command to run Mask2Former on an image, stores its output in /content/output, and store the predictions dictionary in /content/predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-MSRczLhuE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "%cd /content/Mask2Former/demo\n",
        "!python demo.py --config-file /content/Mask2Former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml \\\n",
        "--input /content/input/man.jpg \\\n",
        "--preds_dest /content/predictions/demo \\\n",
        "--output /content/output \\\n",
        "--opts MODEL.WEIGHTS /content/model_final_f07440.pkl\n",
        "%cd /content\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "bDqpsZ4607qZ"
      },
      "outputs": [],
      "source": [
        "def panoptic_segment(img_path, config_file, weights):\n",
        "    '''\n",
        "    Provide the paths to\n",
        "        1. Image\n",
        "        2. Configuration file\n",
        "        3. Weights\n",
        "    Saves output in output and predictions directories\n",
        "    '''\n",
        "    %cd /content/Mask2Former/demo\n",
        "    dst_fn = Path(img_path).stem\n",
        "    dst_fp = os.path.join(preds_dst, dst_fn)\n",
        "    torch.cuda.empty_cache()\n",
        "    cmd = f'python demo.py --config-file {config_file} ' \\\n",
        "          f'--input {img_path} ' \\\n",
        "          f'--preds_dest {dst_fp} ' \\\n",
        "          f'--opts MODEL.WEIGHTS {weights}'\n",
        "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
        "    out, err = p.communicate()\n",
        "    %cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "a-U5DfrJpLVJ"
      },
      "outputs": [],
      "source": [
        "def get_thresh(img):\n",
        "    '''Only consider objects whose size > 1% of image size'''\n",
        "    return img.size[0] * img.size[1] * 0.01\n",
        "\n",
        "def get_bounding_box(img):\n",
        "    '''Get locations of bounding box on an object'''\n",
        "    # region of interest\n",
        "    roi = np.argwhere(img == 255)\n",
        "    # starting point --> top left corner\n",
        "    y1, x1 = roi[:, 0].min(), roi[:, 1].min()\n",
        "    # ending point --> bottom right corner\n",
        "    y2, x2 = roi[:, 0].max(), roi[:, 1].max()\n",
        "    return (x1, y1), (x2, y2)\n",
        "\n",
        "def apply_bounding_box(img):\n",
        "    '''Draw a bounding box on the image'''\n",
        "    start, end = get_bounding_box(img)\n",
        "    rect = cv2.rectangle(img, start, end, (255, 0, 0), 1)\n",
        "    return rect.astype('uint8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Zzm1C-fJl6Jj"
      },
      "outputs": [],
      "source": [
        "def get_metadata(img, labels, instances):\n",
        "    '''\n",
        "    For each object instance in the image,\n",
        "    create a metadata object that contains the\n",
        "        1. Image Supercategory\n",
        "        2. Image Category\n",
        "        3. Array representing the image\n",
        "    (3) is temporarily saved so that it can be\n",
        "    used for attributes prediction\n",
        "    (1) and (2) are obtained through COCO annotations\n",
        "    '''\n",
        "    img_rgb = np.asarray(img)\n",
        "    thresh = get_thresh(img)\n",
        "    img_metadata = list()\n",
        "    global coco_anns\n",
        "\n",
        "    for instance in instances:\n",
        "        if instance['area'] <= thresh or not instance['isthing']:\n",
        "            continue\n",
        "\n",
        "        instance_id = instance['id']\n",
        "        cat_id = instance['category_id']\n",
        "        \n",
        "        metadata = dict()\n",
        "        supercategory = coco_anns[cat_id]['supercategory']\n",
        "        name = coco_anns[cat_id]['name']\n",
        "        metadata['supercategory'] = supercategory\n",
        "        metadata['category'] = name\n",
        "\n",
        "        # get region of interest for current instance\n",
        "        roi = np.where(labels == instance_id, 255, 0).astype('uint8')\n",
        "        (x1, y1), (x2, y2) = get_bounding_box(roi)\n",
        "        crop_rgb = np.zeros(img_rgb.shape, dtype='uint8')\n",
        "        crop_rgb[y1:y2, x1:x2] = img_rgb[y1:y2, x1:x2]\n",
        "        # crop\n",
        "        crop_rgb = crop_rgb[y1:y2, x1:x2]\n",
        "        metadata['image'] = crop_rgb\n",
        "        img_metadata.append(metadata)\n",
        "    return img_metadata\n",
        "\n",
        "def create_base_metadata_from_imgpath(imgpath):\n",
        "    global config_file\n",
        "    global m2f_weights\n",
        "    panoptic_segment(imgpath, config_file, m2f_weights)\n",
        "    filename = os.path.basename(imgpath)\n",
        "    filestem = filename.split('.')[0]\n",
        "    preds_fp = '/content/predictions/' + filestem + \".pkl\"\n",
        "    with open(preds_fp, \"rb\") as f:\n",
        "        preds = pickle.load(f)['panoptic_seg']\n",
        "    os.remove(preds_fp)\n",
        "    img = Image.open(imgpath).convert(\"RGB\")\n",
        "    labels = preds[0].cpu().detach().numpy()\n",
        "    instances = preds[1]\n",
        "    metadata = get_metadata(img, labels, instances)\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "q4Kb7tCOPXHV",
        "outputId": "a04e8f39-056d-447b-b936-0ac502352126"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmetadata = create_metadata_from_imgpath('/content/input/2008_000880.jpg')\\nprint(len(metadata))\\n# Display cropped image of object\\nImage.fromarray(metadata[0]['image'])\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "'''\n",
        "metadata = create_metadata_from_imgpath('/content/input/2008_000880.jpg')\n",
        "print(len(metadata))\n",
        "# Display cropped image of object\n",
        "Image.fromarray(metadata[0]['image'])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHG4o83CRMja"
      },
      "source": [
        "## Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AdVokZ7HWDV5"
      },
      "outputs": [],
      "source": [
        "def get_person_attributes(img):\n",
        "    '''\n",
        "    Return attributes if face found\n",
        "    Else return {}\n",
        "    '''\n",
        "    # if face not found\n",
        "    detectors = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface']\n",
        "    face_found = False\n",
        "    img = np.asarray(img)\n",
        "    for detector in detectors:\n",
        "        try:\n",
        "            img = DeepFace.detectFace(img, detector_backend = detector) \n",
        "            print(f'{detector} failed')  \n",
        "            face_found = True\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(f'{detector} failed')\n",
        "            pass\n",
        "\n",
        "    if not face_found:\n",
        "        return {}\n",
        "\n",
        "    demography = DeepFace.analyze(img, ['age', 'gender', 'race', 'emotion'], enforce_detection=False)\n",
        "\n",
        "    if demography['age'] < 13:\n",
        "        age_cat = 'child'\n",
        "    elif demography['age'] < 20:\n",
        "        age_cat = 'teenager'\n",
        "    elif demography['age'] < 40:\n",
        "        age_cat = 'adult'\n",
        "    elif demography['age'] < 65:\n",
        "        age_cat = 'middle_aged'\n",
        "    else:\n",
        "        age_cat = 'old'\n",
        "\n",
        "    attributes = {\n",
        "        'gender': demography[\"gender\"],\n",
        "        'race': demography[\"dominant_race\"],\n",
        "        'emotion': demography[\"dominant_emotion\"],\n",
        "        'age': age_cat\n",
        "    }\n",
        "\n",
        "    return attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Yc-gX8n0dwsT"
      },
      "outputs": [],
      "source": [
        "def get_dog_attributes(img):\n",
        "    '''\n",
        "    @input: img (PIL Image) of dog\n",
        "    @returns: { \"breed\": \"value\" }\n",
        "    '''\n",
        "    global new_list\n",
        "\n",
        "    # resize to (224, 224)\n",
        "    pred_img_array = cv2.resize(np.asarray(img),((224, 224)))\n",
        "    # resize to (BxHxWxC)\n",
        "    pred_img_array = np.expand_dims(pred_img_array, 0)\n",
        "    # preprocessing for resnet\n",
        "    pred_img_array = preprocess_input(pred_img_array)\n",
        "\n",
        "    # feed the model with the image array for prediction\n",
        "    pred_val = dog_model.predict(pred_img_array)\n",
        "    pred_breed = new_list[np.argmax(pred_val)]\n",
        "    return {\n",
        "        'breed': pred_breed\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "oKO2rU5wju71"
      },
      "outputs": [],
      "source": [
        "def get_car_attributes(img):\n",
        "    transform = T.Compose([T.Resize((400, 400)),\n",
        "                           T.ToTensor(),\n",
        "                           T.Normalize((0.5, 0.5, 0.5), \n",
        "                                       (0.5, 0.5, 0.5))])\n",
        "    \n",
        "    tensor = transform(img).float().unsqueeze(0)\n",
        "    \n",
        "    output = car_model(tensor)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    label = car_anns[predicted.item()]\n",
        "\n",
        "    # separate by spaces\n",
        "    # 1st token -> make\n",
        "    # 2nd to penultimate -> model\n",
        "    # last -> year\n",
        "    # fails if make consists of more than 1 token\n",
        "    tokens = label.split()\n",
        "\n",
        "    return {\n",
        "        'make': tokens[0],\n",
        "        'model': \" \".join(tokens[1:-1]),\n",
        "        'year': int(tokens[-1]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RzEgGyQPz9JG"
      },
      "outputs": [],
      "source": [
        "def add_attributes_to_metadata(metadata):\n",
        "    '''\n",
        "    Apply available image classification models on the metadata\n",
        "    Currently, the following models are used:\n",
        "        1. Person - ethnicity, age, gender, mood\n",
        "        2. Dog - breed\n",
        "        3. Car - make, model, year\n",
        "    '''\n",
        "    for instance in metadata:\n",
        "        img = Image.fromarray(instance['image'])\n",
        "        if instance['category'] == 'person':\n",
        "            instance['metadata'] = get_person_attributes(img)\n",
        "        elif instance['category'] == 'dog':\n",
        "            instance['metadata'] = get_dog_attributes(img)\n",
        "        elif instance['category'] == 'car':\n",
        "            instance['metadata'] = get_car_attributes(img)\n",
        "        else:\n",
        "            instance['metadata'] = {}\n",
        "        # the image is no longer needed\n",
        "        del instance['image']\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-spX-qn7UXZ"
      },
      "source": [
        "## Final Metadata Creation\n",
        "\n",
        "Use `create_metadata_from_imgpath` to create a metadata object.\n",
        "Running it the first time will be slow because some of our models will first download their dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0zF84xX809Wf"
      },
      "outputs": [],
      "source": [
        "def create_metadata_from_imgpath(imgpath):\n",
        "    metadata = create_base_metadata_from_imgpath(imgpath)\n",
        "    metadata_with_attributes = add_attributes_to_metadata(metadata)\n",
        "    return metadata_with_attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract metadata for a single image"
      ],
      "metadata": {
        "id": "ngdn-xO8ISWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "path = '/content/input/man.jpg'\n",
        "create_metadata_from_imgpath(path)\n",
        "'''"
      ],
      "metadata": {
        "id": "0KZiNPwxnDQd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22e1QEdcAPyb"
      },
      "source": [
        "## (Optional) Update PASCAL VOC 2012\n",
        "\n",
        "This section contains the code to create metadata objects for the aforementioned dataset. This section is present because we are using VOC'12 for our project demo.\n",
        "\n",
        "Since we will be annotating our images with their respective `milvus_id`s as well, a JSON file containing the `milvus_id` for each image name is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncRkA5CcAtdg"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/AttributeDetection/mid_filename.json'\n",
        "with open(filepath) as f:\n",
        "    imgname2mid = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvfXYwf3Bgff"
      },
      "source": [
        "### Load Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVkI3dC-Es5X"
      },
      "outputs": [],
      "source": [
        "kaggle_creds = '/content/AttributeDetection/kaggle.json'\n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp $kaggle_creds ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download huanghanchina/pascal-voc-2012\n",
        "!unzip -qq pascal-voc-2012.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czBZ0wE_FYgJ"
      },
      "outputs": [],
      "source": [
        "IMGS_DIR = '/content/VOC2012/JPEGImages'\n",
        "images = os.listdir(IMGS_DIR)\n",
        "\n",
        "pascal_metadata = list()\n",
        "start = ...\n",
        "for i in trange(start, len(images)):\n",
        "    img = images[i]\n",
        "    img_path = os.path.join(IMGS_DIR, img)\n",
        "    try:\n",
        "        metadata = create_metadata_from_imgpath(img_path)\n",
        "    except:\n",
        "        with open(\"/content/drive/MyDrive/AttributeDetection/errors_dawood3.txt\", \"a\") as f:\n",
        "            f.write(str(i)+\"\\n\")\n",
        "        continue\n",
        "    else:\n",
        "        mid = imgname2mid[img]   \n",
        "        metadata.insert(0, { \"milvus_id\": mid, \"name\": img })\n",
        "        pascal_metadata.append(metadata)\n",
        "        with open(\"/content/drive/MyDrive/AttributeDetection/metadata_dawood3.json\", \"w\") as f:\n",
        "            json.dump(pascal_metadata, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Metadata Creation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}